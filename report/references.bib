@article{vcerny1985thermodynamical,
  title={Thermodynamical approach to the traveling salesman problem: An efficient simulation algorithm},
  author={{\v{C}}ern{\`y}, Vladim{\'\i}r},
  journal={Journal of optimization theory and applications},
  volume={45},
  number={1},
  pages={41--51},
  year={1985},
  publisher={Springer}
}
@incollection{balas1980set,
  title={Set covering algorithms using cutting planes, heuristics, and subgradient optimization: a computational study},
  author={Balas, Egon and Ho, Andrew},
  booktitle={Combinatorial optimization},
  pages={37--60},
  year={1980},
  publisher={Springer}
}
@inproceedings{chierichetti2010max,
  title={Max-cover in map-reduce},
  author={Chierichetti, Flavio and Kumar, Ravi and Tomkins, Andrew},
  booktitle={Proceedings of the 19th international conference on World wide web},
  pages={231--240},
  year={2010},
  organization={ACM}
}
@article{grossman1997computational,
  title={Computational experience with approximation algorithms for the set covering problem},
  author={Grossman, Tal and Wool, Avishai and others},
  journal={European Journal of Operational Research},
  volume={101},
  number={1},
  pages={81--92},
  year={1997}
}
@book{kearns1994introduction,
  title={An introduction to computational learning theory},
  author={Kearns, Michael J and Vazirani, Umesh Virkumar and Vazirani, Umesh},
  year={1994},
  publisher={MIT press}
}
@inproceedings{saha2009maximum,
  title={On maximum coverage in the streaming model \& application to multi-topic blog-watch},
  author={Saha, Barna and Getoor, Lise},
  booktitle={Proceedings of the 2009 siam international conference on data mining},
  pages={697--708},
  year={2009},
  organization={SIAM}
}
@article{johnson1974approximation,
  title={Approximation algorithms for combinatorial problems},
  author={Johnson, David S},
  journal={Journal of computer and system sciences},
  volume={9},
  number={3},
  pages={256--278},
  year={1974},
  publisher={Elsevier}
}
@techreport{Kirkpatrick1983,
author = {Kirkpatrick, S and Gelatt, C. D. and Vecchi, M. P.},
booktitle = {New Series},
file = {:home/dominik/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kirkpatrick, Gelatt, Vecchi - 1983 - Optimization by Simulated Annealing(3).pdf:pdf},
number = {4598},
pages = {671--680},
title = {{Optimization by Simulated Annealing}},
url = {https://pdfs.semanticscholar.org/beb2/1ee4a3721484b5d2c7ad04e6babd8d67af1d.pdf},
volume = {220},
publisher={American Association for the Advancement of Science},
year = {1983}
}
@incollection{Karp2010,
address = {Berlin, Heidelberg},
author = {Karp, Richard M.},
booktitle = {50 Years of Integer Programming 1958-2008},
doi = {10.1007/978-3-540-68279-0_8},
pages = {219--241},
publisher = {Springer Berlin Heidelberg},
title = {{Reducibility Among Combinatorial Problems}},
url = {http://link.springer.com/10.1007/978-3-540-68279-0{\_}8},
year = {2010}
}
@article{Jacobs1995,
abstract = {In this note we describe a local-search heuristic (LSH) for large non-unicost set-covering problems (SCPs). The new heuristic is based on the simulated annealing algorithm and uses an improvement routine designed to provide low-cost solutions within a reasonable amount of CPU time. The solution costs associated with the LSH compared very favorably to the best previously published solution costs for 20 large SCPs taken from the literature. In particular, the LSH yielded new benchmark solutions for 17 of the 20 test problems. We also report that, for SCPs where column cost is correlated with column coverage, the new heuristic provides solution costs competitive with previously published results for comparable problems. {\textcopyright} 1995 John Wiley {\&} Sons, Inc.},
author = {Jacobs, Larry W. and Brusco, Michael J.},
doi = {10.1002/1520-6750(199510)42:7<1129::AID-NAV3220420711>3.0.CO;2-M},
file = {:home/dominik/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jacobs, Brusco - 1995 - Note A local‐search heuristic for large set‐covering problems.pdf:pdf},
issn = {15206750},
journal = {Naval Research Logistics (NRL)},
title = {{Note: A local-search heuristic for large set-covering problems}},
year = {1995}
}
@article{Indyk,
abstract = {We study the classic set cover problem from the perspec-tive of sub-linear algorithms. Given access to a collec-tion of m sets over n elements in the query model, we show that sub-linear algorithms derived from existing techniques have almost tight query complexities. On one hand, first we show an adaptation of the streaming algorithm presented in [17] to the sub-linear query model, that returns an $\alpha$-approximate cover using O(m(n/k) 1/($\alpha$−1) + nk) queries to the input, where k denotes the value of a minimum set cover. We then complement this upper bound by proving that for lower values of k, the required number of queries is Ω(m(n/k) 1/(2$\alpha$)), even for estimating the optimal cover size. Moreover, we prove that even checking whether a given collection of sets covers all the elements would require Ω(nk) queries. These two lower bounds provide strong evidence that the upper bound is almost tight for certain values of the parameter k. On the other hand, we show that this bound is not optimal for larger values of the parameter k, as there ex-ists a (1 + $\epsilon$)-approximation algorithm with O(mn/k$\epsilon$ 2) queries. We show that this bound is essentially tight for sufficiently small constant $\epsilon$, by establishing a lower bound of Ω(mn/k) query complexity. Our lower-bound results follow by carefully design-ing two distributions of instances that are hard to dis-tinguish. In particular, our first lower bound involves a probabilistic construction of a certain set system with a minimum set cover of size $\alpha$k, with the key property that a small number of " almost uniformly distributed " modifications can reduce the minimum set cover size down to k. Thus, these modifications are not detectable unless a large number of queries are asked. We believe that our probabilistic construction technique might find applications to lower bounds for other combinatorial op-timization problems. 1 Introduction Set Cover is a classic combinatorial optimization prob-lem, in which we are given a set (universe) of n el-ements U = {\{}e 1 , {\textperiodcentered} {\textperiodcentered} {\textperiodcentered} , e n {\}} and a collection of m sets F = {\{}S 1 , {\textperiodcentered} {\textperiodcentered} {\textperiodcentered} , S m {\}}. The goal is to find a set cover of U, i.e., a collection of sets in F whose union is U, of min-imum size. Set Cover is a well-studied problem with applications in operations research [16], information re-trieval and data mining [32], learning theory [19], web host analysis [9], and many others. Recently, this prob-lem and other related coverage problems have gained a lot of attention in the context of massive data sets, e.g., streaming model [32, 12, 10, 17, 7, 3, 24, 2, 5, 18] or map reduce model [22, 25, 4]. Although the problem of finding an optimal solution is NP-complete, a natural greedy algorithm which iteratively picks the " best " remaining set (the set that covers the most number of uncovered elements) is widely used. The algorithm finds a solution of size at most k ln n where k is the optimum cover size, and can be implemented to run in time linear in the input size. However, the input size itself could be as large as $\Theta$(mn), so for large data sets even reading the input might be infeasible. This raises a natural question: is it possible to solve minimum set cover in sub-linear time? This question was previously addressed in [28, 33], who showed that one can design constant running-time algorithms by simulating the greedy algorithm, under the assumption that the sets are of constant size and each element occurs in a constant number of sets. However, those constant-time algorithms have a few drawbacks: they only provide a mixed multiplicative/additive guarantee (the output cover size is guaranteed to be at most k {\textperiodcentered} ln n + n), the dependence of their running times on the maximum set size is exponential, and they only output the (approximate) minimum set cover size, not the cover itself. From a different perspective, [20] (building on [15]) showed that an O(1)-approximate solution to the fractional version of the problem can be found in},
author = {Indyk, Piotr and Mahabadi, Sepideh and Rubinfeld, Ronitt and Vakilian, Ali and Yodpinyanee, Anak},
file = {:home/dominik/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Indyk et al. - Unknown - Set Cover in Sub-linear Time.pdf:pdf},
title = {{Set Cover in Sub-linear Time *}},
url = {http://www.mit.edu/{~}vakilian/publication/IMRVY18.pdf},
year = {2018}
}
@article{Chang2007,
abstract = {According to previous research of Chang et al. [Chang, P. C., Chen, S. H., {\&} Lin, K. L. (2005b). Two phase sub-population genetic algorithm for parallel machine scheduling problem. Expert Systems with Applications, 29(3), 705–712], the sub-population genetic algorithm (SPGA) is effective in solving multiobjective scheduling problems. Based on the pioneer efforts, this research proposes a mining gene structure technique integrated with the SPGA. The mining problem of elite chromosomes is formulated as a linear assignment problem and a greedy heuristic using threshold to eliminate redundant information. As a result, artificial chromosomes are created according to this gene mining procedure and these artificial chromosomes will be reintroduced into the evolution process to improve the efficiency and solution quality of the procedure. In addition, to further increase the quality of the artificial chromosome, a dynamic threshold procedure is developed and the flowshop scheduling problems are applied as a benchmark problem for testing the developed algorithm. Extensive tests in the flow-shop scheduling problem show that the proposed approach can improve the performance of SPGA significantly.},
author = {Chang, Pei-Chann and Chen, Shih-Hsin and Liu, Chen-Hao},
doi = {10.1016/J.ESWA.2006.06.019},
issn = {0957-4174},
journal = {Expert Systems with Applications},
month = {oct},
number = {3},
pages = {762--771},
publisher = {Pergamon},
title = {{Sub-population genetic algorithm with mining gene structures for multiobjective flowshop scheduling problems}},
url = {https://www.sciencedirect.com/science/article/pii/S0957417406002028},
volume = {33},
year = {2007}
}
@article{Langmead2009,
abstract = {Bowtie: short-read alignment {\textless}p{\textgreater}Bowtie: a new ultrafast memory-efficient tool for the alignment of short DNA sequence reads to large genomes.{\textless}/p{\textgreater} Abstract Bowtie is an ultrafast, memory-efficient alignment program for aligning short DNA sequence reads to large genomes. For the human genome, Burrows-Wheeler indexing allows Bowtie to align more than 25 million reads per CPU hour with a memory footprint of approximately 1.3 gigabytes. Bowtie extends previous Burrows-Wheeler techniques with a novel quality-aware backtracking algorithm that permits mismatches. Multiple processor cores can be used simultaneously to achieve even greater alignment speeds. Bowtie is open source http://bowtie.cbcb.umd.edu. Rationale Improvements in the efficiency of DNA sequencing have both broadened the applications for sequencing and dramatically increased the size of sequencing datasets. Technologies from Illumina (San Diego, CA, USA) and Applied Biosystems (Fos-ter City, CA, USA) have been used to profile methylation patterns (MeDIP-Seq) [1], to map DNA-protein interactions (ChIP-Seq) [2], and to identify differentially expressed genes (RNA-Seq) [3] in the human genome and other species. The Illumina instrument was recently used to re-sequence three human genomes, one from a cancer patient and two from previously unsequenced ethnic groups [4-6]. Each of these studies required the alignment of large numbers of short DNA sequences ('short reads') onto the human genome. For example , two of the studies [4,5] used the short read alignment tool Maq [7] to align more than 130 billion bases (about 45× coverage) of short Illumina reads to a human reference genome in order to detect genetic variations. The third human re-sequencing study [6] used the SOAP program [8] to align more than 100 billion bases to the reference genome. In addition to these projects, the 1,000 Genomes project is in the process of using high-throughput sequencing instruments to sequence a total of about six trillion base pairs of human DNA [9]. With existing methods, the computational cost of aligning many short reads to a mammalian genome is very large. For example, extrapolating from the results presented here in Tables 1 and 2, one can see that Maq would require more than 5 central processing unit (CPU)-months and SOAP more than 3 CPU-years to align the 140 billion bases from the study by Ley and coworkers [5]. Although using Maq or SOAP for this purpose has been shown to be feasible by using multiple CPUs, there is a clear need for new tools that consume less time and computational resources. Maq and SOAP take the same basic algorithmic approach as other recent read mapping tools such as RMAP [10], ZOOM [11], and SHRiMP [12]. Each tool builds a hash table of short oligomers present in either the reads (SHRiMP, Maq, RMAP, and ZOOM) or the reference (SOAP). Some employ recent theoretical advances to align reads quickly without sacrificing sensitivity. For example, ZOOM uses 'spaced seeds' to significantly outperform RMAP, which is based on a simpler algo-The electronic version of this article is the complete one and can be found online at},
author = {Langmead, Ben and Trapnell, Cole and Pop, Mihai and Salzberg, Steven L},
doi = {10.1186/gb-2009-10-3-r25},
file = {:home/dominik/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Langmead et al. - 2009 - Ultrafast and memory-efficient alignment of short DNA sequences to the human genome.pdf:pdf},
journal = {Genome Biology},
number = {3},
pages = {25},
title = {{Ultrafast and memory-efficient alignment of short DNA sequences to the human genome}},
url = {http://genomebiology.com/2009/10/3/R25},
volume = {10},
year = {2009}
}
@article{Edgar2004,
abstract = {We describe MUSCLE, a new computer program for creating multiple alignments of protein sequences. Elements of the algorithm include fast distance estimation using kmer counting, progressive alignment using a new profile function we call the log-expectation score, and refinement using tree-dependent restricted partitioning. The speed and accuracy of MUSCLE are compared with T-Coffee, MAFFT and CLUSTALW on four test sets of reference alignments: BAliBASE, SABmark, SMART and a new benchmark, PREFAB. MUSCLE achieves the highest, or joint highest, rank in accuracy on each of these sets. Without refinement, MUSCLE achieves average accuracy statistically indistinguishable from T-Coffee and MAFFT, and is the fastest of the tested methods for large numbers of sequences, aligning 5000 sequences of average length 350 in 7 min on a current desktop computer. The MUSCLE program, source code and PREFAB test data are freely available at http://www.drive5. com/muscle.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Edgar, Robert C.},
doi = {10.1093/nar/gkh340},
eprint = {NIHMS150003},
file = {:home/dominik/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Edgar - 2004 - MUSCLE Multiple sequence alignment with high accuracy and high throughput.pdf:pdf},
isbn = {1362-4962 (Electronic) 0305-1048 (Linking)},
issn = {03051048},
journal = {Nucleic Acids Research},
pmid = {15034147},
title = {{MUSCLE: Multiple sequence alignment with high accuracy and high throughput}},
year = {2004}
}
@article{Zorbas2010,
abstract = {To achieve power efficient monitoring of targets by sensor networks, various coverage algorithms have been proposed. These algorithms divide the sensor nodes into cover sets, where each cover set is capable of monitoring all targets. Generating the maximum number of cover sets has been proven to be an NP-complete problem and, thus, algorithms producing sub-optimal solutions have been proposed. In this paper we present a novel and efficient coverage algorithm, that can produce both disjoint cover sets, i.e. cover sets with no common sensor nodes, as well as non-disjoint cover sets. While searching for the best sensor to include in a cover set, our algorithm uses a cost function that takes into account the monitoring capabilities of a sensor, its association with poorly monitored targets, but also the sensor's remaining battery life. Through simulations, we show that the proposed algorithm outperforms similar heuristic algorithms found in the literature, producing collections of cover sets of optimal (or near-optimal) size. The increased availability offered by these cover sets along with the short execution time of the proposed algorithm make it desirable for a wide range of node deployment environments. {\textcopyright} 2009 Elsevier B.V. All rights reserved.},
author = {Zorbas, Dimitrios and Glynos, Dimitris and Kotzanikolaou, Panayiotis and Douligeris, Christos},
doi = {10.1016/j.adhoc.2009.10.003},
file = {:home/dominik/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zorbas et al. - 2010 - Solving coverage problems in wireless sensor networks using cover sets.pdf:pdf},
issn = {15708705},
journal = {Ad Hoc Networks},
keywords = {Centralised algorithm,Cover sets,Target coverage,Wireless sensor networks},
title = {{Solving coverage problems in wireless sensor networks using cover sets}},
year = {2010}
}
@article{Stratulat2001,
abstract = {Cover set induction is known as a proof method that keeps the advantages of explicit induction and proof by consistency. Most implicit induction proof procedures are defined in a cover set induction framework. Contextual cover set (CCS) is a new concept that fully characterizes explicit induction schemes, such as the cover sets, and many simplification techniques as those specific to the "proof by consistency" approach. Firstly, we present an abstract inference system uniformly defined in terms of contextual cover sets as our general framework to build implicit induction provers. Then, we show that it generalizes existing cover set induction procedures. This paper also contributes to the general problem of assembling reasoning systems in a sound manner. Elementary CCSs are generated by reasoning modules that implement various simplification techniques defined for a large class of deduction mechanisms such as rewriting, conditional rewriting and resolution-based methods for clauses. We present a generic and sound integration schema of reasoning modules inside our procedure together with a simple methodology for improvements and incremental sound extensions of the concrete proof procedures. As a case study, the inference system of the SPIKE theorem prover has been shown to be an instance of the abstract inference system integrating reasoning modules based on rewriting techniques defined for conditional theories. Our framework allows for modular and incremental sound extensions of SPIKE when new reasoning techniques are proposed. An extension of the prover, incorporating inductive semantic subsumption techniques, has proved the correctness of the MJRTY algorithm by performing a combination of arithmetic and inductive reasoning. {\textcopyright} 2001 Academic Press.},
author = {Stratulat, Sorin},
doi = {10.1006/jsco.2000.0469},
file = {:home/dominik/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Stratulat - 2001 - A general framework to build contextual cover set induction provers.pdf:pdf},
issn = {07477171},
journal = {Journal of Symbolic Computation},
title = {{A general framework to build contextual cover set induction provers}},
year = {2001}
}
@article{Bansal2006,
abstract = {We consider the following problem: The Santa Claus has n presents that he wants to distribute among m kids. Each kid has an arbitrary value for each present. Let pij be the value that kid i has for present j. The Santa's goal is to distribute presents in such a way that the least lucky kid is as happy as possible, i.e he tries to maximize mini=1,...,m P j∈S i pij where Si is a set of presents received by the i-th kid. Our main result is an O(log log m/ log log log m) approximation algorithm for the restricted assignment case of the problem when pij ∈ {\{}pj, 0{\}} (i.e. when present j has either value pj or 0 for each kid). Our algorithm is based on rounding a certain natural exponentially large linear programming relaxation usually referred to as the configuration LP. We also show that the configuration LP has an integrality gap of $\Omega$(m 1/2) in the general case, when pij can be arbitrary.},
author = {Bansal, Nikhil and Sviridenko, Maxim},
file = {:home/dominik/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bansal, Sviridenko - 2006 - The Santa Claus Problem.pdf:pdf},
keywords = {F22 [Analysis of Algorithms and Problem Complex-it,Theory Keywords Scheduling,approximation algorithms,maximin,resource allocation,unrelated machines},
title = {{The Santa Claus Problem}},
year = {2006}
}
@article{Ben-Ari1998,
author = {Ben-Ari, Mordechai},
doi = {10.1002/(SICI)1096-9128(199805)10:6<485::AID-CPE329>3.0.CO;2-2},
file = {:home/dominik/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ben-Ari - 1998 - How to solve the Santa Claus problem.pdf:pdf},
issn = {10403108},
journal = {Concurrency Practice and Experience},
title = {{How to solve the Santa Claus problem}},
year = {1998}
}
@article{Lee1991,
abstract = {In this paper we consider the problem of scheduling n independent jobs on m identical machines in order to minimize the makespan, the total finishing time. The jobs are available at time zero, but some machines may not be available at time zero. This problem is a generalization of the classical multiprocessor scheduling problem, where all machines are available simultaneously at time zero. We first show that the makespan obtained by applying the longest processing time (LPT) algorithm to our generalized problem is always within ( 3 2- 1 2m)M*and the bound is tight. We then provide a modified LPT (MLPT) algorithm and show that the makespan obtained by MLPT is bounded by 4 3M*. {\textcopyright} 1991.},
author = {Lee, Chung Yee},
doi = {10.1016/0166-218X(91)90013-M},
file = {:home/dominik/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lee - 1991 - Parallel machines scheduling with nonsimultaneous machine available time.pdf:pdf},
issn = {0166218X},
journal = {Discrete Applied Mathematics},
title = {{Parallel machines scheduling with nonsimultaneous machine available time}},
year = {1991}
}
@article{Deb2002,
abstract = {Multiobjective evolutionary algorithms (EAs) that use nondominated sorting and sharing have been criticized mainly for their: 1) computational complexity (where is the number of objectives and is the population size); 2) nonelitism approach; and 3) the need for specifying a sharing parameter. In this paper, we suggest a nondominated sorting-based multiobjective EA (MOEA), called nondominated sorting genetic algorithm II (NSGA-II), which alleviates all the above three difficulties. Specifically, a fast nondominated sorting approach with computational complexity is presented. Also, a selection operator is presented that creates a mating pool by combining the parent and offspring populations and selecting the best (with respect to fitness and spread) solutions. Simulation results on difficult test problems show that the proposed NSGA-II, in most problems, is able to find much better spread of solutions and better convergence near the true Pareto-optimal front compared to Pareto-archived evolution strategy and strength-Pareto EA-two other elitist MOEAs that pay special attention to creating a diverse Pareto-optimal front. Moreover, we modify the definition of dominance in order to solve constrained multiobjective problems efficiently. Simulation results of the constrained NSGA-II on a number of test problems, including a five-objective seven-constraint nonlinear problem, are compared with another constrained multiobjective optimizer and much better performance of NSGA-II is observed. Index Terms-Constraint handling, elitism, genetic algorithms, multicriterion decision making, multiobjective optimization, Pareto-optimal solutions.},
author = {Deb, Kalyanmoy and Pratap, Amrit and Agarwal, Sameer and Meyarivan, T},
file = {:home/dominik/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Deb et al. - 2002 - A Fast and Elitist Multiobjective Genetic Algorithm NSGA-II.pdf:pdf},
journal = {IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION},
number = {2},
title = {{A Fast and Elitist Multiobjective Genetic Algorithm: NSGA-II}},
volume = {6},
year = {2002}
}
@article{Feo1995,
abstract = {Today, a variety of heuristic approaches are available to the operations research practitioner. One methodology that has a strong intuitive appeal, a prominent empirical track record, and is trivial to efficiently implement on parallel processors is GRASP (Greedy Randomized Adaptive Search Procedures). GRASP is an iterative randomized sampling technique in which each iteration provides a solution to the problem at hand. The incumbent solution over all GRASP iterations is kept as the final result. There are two phases within each GRASP iteration: the first intelligently constructs an initial solution via an adaptive randomized greedy function; the second applies a local search procedure to the constructed solution in hope of finding an improvement. In this paper, we define the various components comprising a GRASP and demonstrate, step by step, how to develop such heuristics for combinatorial optimization problems. Intuitive justifications for the observed empirical behavior of the methodology are discussed. The paper concludes with a brief literature review of GRASP implementations and mentions two industrial applications.},
author = {Feo, Thomas A and Resende, Mauricio G C},
file = {:home/dominik/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Feo, Resende - 1995 - Greedy Randomized Adaptive Search Procedures.pdf:pdf},
journal = {Journal of Global Optimization},
keywords = {Combinatorial optimization,GRASP,computer implementation,search heuristic},
pages = {109--133},
publisher = {KluwerAcademic Publishers},
title = {{Greedy Randomized Adaptive Search Procedures}},
volume = {6},
year = {1995}
}
@article{Feo1995a,
abstract = {Today, a variety of heuristic approaches are available to the operations research practitioner. One methodology that has a strong intuitive appeal, a prominent empirical track record, and is trivial to efficiently implement on parallel processors is GRASP (Greedy Randomized Adaptive Search Procedures). GRASP is an iterative randomized sampling technique in which each iteration provides a solution to the problem at hand. The incumbent solution over all GRASP iterations is kept as the final result. There are two phases within each GRASP iteration: the first intelligently constructs an initial solution via an adaptive randomized greedy function; the second applies a local search procedure to the constructed solution in hope of finding an improvement. In this paper, we define the various components comprising a GRASP and demonstrate, step by step, how to develop such heuristics for combinatorial optimization problems. Intuitive justifications for the observed empirical behavior of the methodology are discussed. The paper concludes with a brief literature review of GRASP implementations and mentions two industrial applications.},
author = {Feo, Thomas A and Resende, Mauricio G C},
file = {:home/dominik/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Feo, Resende - 1995 - Greedy Randomized Adaptive Search Procedures.pdf:pdf},
journal = {Journal of Global Optimization},
keywords = {Combinatorial optimization,GRASP,computer implementation,search heuristic},
pages = {109--133},
publisher = {KluwerAcademic Publishers},
title = {{Greedy Randomized Adaptive Search Procedures}},
volume = {6},
year = {1995}
}
@article{Alon2003,
abstract = {Let X = {\{}1, 2,. .. , n{\}} be a ground set of n elements, and let S be a family of subsets of X, |S| = m, with a positive cost cS associated with each S ∈ S. Consider the following online version of the set cover problem , described as a game between an algorithm and an adversary. An adversary gives elements to the algorithm from X one-by-one. Once a new element is given, the algorithm has to cover it by some set of S containing it. We assume that the elements of X and the members of S are known in advance to the algorithm, however, the set X ⊆ X of elements given by the adversary is not known in advance to the algorithm. (In general, X may be a strict subset of X.) The objective is to minimize the total cost of the sets chosen by the algorithm. Let C denote the family of sets in S that the algorithm chooses. At the end of the game the adversary also produces (off-line) a family of sets COP T that covers X. The performance of the algorithm is the ratio between the cost of C and the cost of COP T. The maximum ratio, taken over all input sequences, is the competitive ratio of the algorithm. We present an O(log m log n) competitive deterministic algorithm for the problem, and establish a nearly matching Ω log n log m log log m+log log n lower bound for all interesting values of m and n. The techniques used are motivated by similar techniques developed in computational learning theory for online prediction (e.g., the WINNOW algorithm) together with a novel way of converting the fractional solution they supply into a deterministic online algorithm.},
author = {Alon, Noga and Awerbuch, Baruch and Azar, Yossi and Buchbinder, Niv and Seffi, Joseph ( and Naor, )},
file = {:home/dominik/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Alon et al. - 2003 - The Online Set Cover Problem.pdf:pdf},
keywords = {Competitive Analysis,Derandomization,F22 [Analysis of Algorithms and Problem Complex-it,Ran-domized Rounding,Set-Cover,Theory Keywords On-line Algorithms},
title = {{The Online Set Cover Problem}},
year = {2003}
}
@article{Feige1998,
abstract = {Abstract. Given a collection {\^{}} of subsets of S 5 {\{}1, . . . , n{\}}, set cover is the problem of selecting as few as possible subsets from {\^{}} such that their union covers S, and max k-cover is the problem of selecting k subsets from {\^{}} such that their union has maximum cardinality. Both these problems are NP-hard. We prove that (1 2 o(1)) ln n is a threshold below which set cover cannot be approximated efficiently, unless NP has slightly superpolynomial time algorithms. This closes the gap (up to low-order terms) between the ratio of approximation achievable by the greedy algorithm (which is (1 2 o(1)) ln n), and previous results of Lund and Yannakakis, that showed hardness of approximation within a ratio of (log2 n)/ 2 . 0.72 ln n. For max k-cover, we show an approximation threshold of (1 2 1/e) (up to low-order terms), under the assumption that P Þ NP.},
author = {Feige, Uriel},
doi = {10.1145/285055.285059},
file = {:home/dominik/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Feige - 1998 - A threshold of ln n for approximating set cover.pdf:pdf},
isbn = {9781605580470},
issn = {00045411},
journal = {Journal of the ACM},
title = {{A threshold of ln n for approximating set cover}},
year = {1998}
}
@article{Deb2002a,
abstract = {Multiobjective evolutionary algorithms (EAs) that use nondominated sorting and sharing have been criticized mainly for their: 1) computational complexity (where is the number of objectives and is the population size); 2) nonelitism approach; and 3) the need for specifying a sharing parameter. In this paper, we suggest a nondominated sorting-based multiobjective EA (MOEA), called nondominated sorting genetic algorithm II (NSGA-II), which alleviates all the above three difficulties. Specifically, a fast nondominated sorting approach with computational complexity is presented. Also, a selection operator is presented that creates a mating pool by combining the parent and offspring populations and selecting the best (with respect to fitness and spread) solutions. Simulation results on difficult test problems show that the proposed NSGA-II, in most problems, is able to find much better spread of solutions and better convergence near the true Pareto-optimal front compared to Pareto-archived evolution strategy and strength-Pareto EA-two other elitist MOEAs that pay special attention to creating a diverse Pareto-optimal front. Moreover, we modify the definition of dominance in order to solve constrained multiobjective problems efficiently. Simulation results of the constrained NSGA-II on a number of test problems, including a five-objective seven-constraint nonlinear problem, are compared with another constrained multiobjective optimizer and much better performance of NSGA-II is observed. Index Terms-Constraint handling, elitism, genetic algorithms, multicriterion decision making, multiobjective optimization, Pareto-optimal solutions.},
author = {Deb, Kalyanmoy and Pratap, Amrit and Agarwal, Sameer and Meyarivan, T},
file = {:home/dominik/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Deb et al. - 2002 - A Fast and Elitist Multiobjective Genetic Algorithm NSGA-II.pdf:pdf},
journal = {IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION},
number = {2},
title = {{A Fast and Elitist Multiobjective Genetic Algorithm: NSGA-II}},
volume = {6},
year = {2002}
}
@article{Fujito2000,
abstract = {The main problem considered is submodular set cover, the problem of minimizing a linear function under a nondecreasing submodular constraint, which generalizes both well-known set cover and minimum matroid base problems. The problem is NP-hard, and two natural greedy heuristics are introduced along with analysis of their performance. As applications of these heuristics we consider various special cases of submodular set cover, including partial cover variants of set cover and vertex cover, and node-deletion problems for hereditary and matroidal properties. An approximation bound derived for each of them is either matching or generalizing the best existing bounds.},
author = {Fujito, Toshihiro},
file = {:home/dominik/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fujito - 2000 - Approximation algorithms for submodular set cover with applications.pdf:pdf},
issn = {09168532},
journal = {IEICE Transactions on Information and Systems},
keywords = {Approximation algorithms,Greedy heuristics,Set cover,Submodular function},
title = {{Approximation algorithms for submodular set cover with applications}},
year = {2000}
}
@article{Kao,
abstract = {In this paper, we introduce a genetic algorithm approach for set covering problems. Since the set covering problems are constrained optimization problems we utilize a new penalty function to handle the constraints. In addition, we propose a mutation operator which can approach the optima from both sides of feasible/infeasible borders. We experiment with our genetic algorithm to solve several instances of computationally difficult set covering problems that arise from computing the l-width of the incidence matrix of Steiner triple systems. We have found better solutions than the currently best-known solutions for two large test problems.},
author = {Kao, Cheng-Yan and Horng, Jorng-Tzong},
file = {:home/dominik/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kao, Horng - Unknown - A Genetic Algorithm Approach for Set Covering Problems.pdf:pdf},
title = {{A Genetic Algorithm Approach for Set Covering Problems}},
year = {1994}
}
@inproceedings{Cormode,
  title={Set cover algorithms for very large datasets},
  author={Cormode, Graham and Karloff, Howard and Wirth, Anthony},
  booktitle={Proceedings of the 19th ACM international conference on Information and knowledge management},
  pages={479--488},
  year={2010},
  organization={ACM}
}
@article{Laporte1992,
abstract = {In this paper, some of the main known algorithms for the traveling salesman problem are surveyed. The paper is organized as follows: 1) definition; 2) applications; 3) complexity analysis; 4) exact algorithms; 5) heuristic algorithms; 6) conclusion.},
author = {Laporte, Gilbert},
file = {:home/dominik/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Laporte - 1992 - The Traveling Salesman Problem An overview of exact and approximate algorithms.pdf:pdf},
journal = {European Journal of Operational Research},
keywords = {PII: 0377-2217(92)90138-Y},
pages = {231--247},
title = {{The Traveling Salesman Problem: An overview of exact and approximate algorithms}},
volume = {59},
year = {1992}
}
@article{Mataija2016,
abstract = {The goal of this paper is to optimize delivering of packages at five randomly chosen addresses in the city of Rijeka. This problem is also known as the Travelling Salesman Problem and it is an NP hard problem. To achieve this goal, the concepts of a Hamilton path and cycle, as well as a Hamilton graph are defined. The theoretical basis for the branch and bound method is also given. The use of this method in the process of finding a solution for a problem is provided at the end of this paper.},
author = {Mataija, M and {\v{S}}egi{\'{c}}, M Rakamari{\'{c}} and Jozi{\'{c}}, F},
file = {:home/dominik/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mataija, {\v{S}}egi{\'{c}}, Jozi{\'{c}} - 2016 - Solving the travelling salesman problem using the Branch and.pdf:pdf},
keywords = {Branch and Bound Method,Hamilton cycle,Hamilton path,NP complete problem,NP hard problem,Travelling Salesman Problem},
number = {1},
pages = {259--270},
title = {{Solving the travelling salesman problem using the Branch and}},
volume = {4},
year = {2016}
}
@article{Kolesar1967,
abstract = {A branch and bound algorithm for solution of the "knapsack problem," max E vzix where E wixi {\textless} W and xi = 0, 1, is presented which can obtain either optimal or approximate solutions. Some characteristics of the algorithm are discussed and computational experience is presented. Problems involving 50 items from which approximately 25 items were loaded were solved in an average of 0.07 minutes each by a coded version of this algorithm for the IBM 7094 computer.},
author = {Kolesar, Peter J},
file = {:home/dominik/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kolesar - 1967 - A BRANCH AND BOUND ALGORITHM FOR THE KNAPSACK PROBLEM t.pdf:pdf},
journal = {MANAGEMENT SCIENCE},
number = {9},
title = {{A BRANCH AND BOUND ALGORITHM FOR THE KNAPSACK PROBLEM *t}},
volume = {13},
year = {1967}
}
@article{Schaub2014,
abstract = {The analysis of complex networks has so far revolved mainly around the role of nodes and communities of nodes. However, the dynamics of interconnected systems is often focalized on edge processes, and a dual edge-centric perspective can often prove more natural. Here we present graph-theoretical measures to quantify edge-to-edge relations inspired by the notion of flow redistribution induced by edge failures. Our measures, which are related to the pseudo-inverse of the Laplacian of the network, are global and reveal the dynamical interplay between the edges of a network, including potentially non-local interactions. Our framework also allows us to define the embeddedness of an edge, a measure of how strongly an edge features in the weighted cuts of the network. We showcase the general applicability of our edge-centric framework through analyses of the Iberian power grid, traffic flow in road networks, and the C. elegans neuronal network},
archivePrefix = {arXiv},
arxivId = {arXiv:1303.6241v1},
author = {Schaub, Michael T. and Lehmann, J{\"{o}}rg and Yaliraki, Sophia N. and Barahona, Mauricio},
doi = {10.1017/nws.2014.4},
eprint = {arXiv:1303.6241v1},
file = {:home/dominik/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Schaub et al. - 2014 - Structure of complex networks Quantifying edge-to-edge relations by failure-induced flow redistribution.pdf:pdf},
isbn = {2050-1250},
issn = {20501250},
journal = {Network Science},
keywords = {C. elegans,community structure,edge-centralities,edge-failures,edge-to-edge relations,flow redistribution,power grids,pseudoinverse of the Laplacian,traffic networks},
title = {{Structure of complex networks: Quantifying edge-to-edge relations by failure-induced flow redistribution}},
year = {2014}
}
@article{Abraham2018,
abstract = {In this paper, a new mathematical framework is proposed for maximizing the self-cleaning capacity (SCC) of drinking water distribution systems by controlling the diurnal peak flow velocities in the pipes under normal operation. This is achieved through an optimal change of the network connectivity (topology). This paper proposes an efficient algorithm for the network analysis of valve closures, which allows enforcing favorable changes in the flow velocities for maximizing the SCC by determining an optimal set of links to isolate in the forming of a more branched network, while concurrently satisfying the hydraulic and regulatory pressure constraints at the demand nodes. Multiple stochastic demands from an end-use demand model are generated to test the robustness in the improved SCC for the modified network connectivity under changing demand. An operational network model is used to demonstrate the efficacy of the proposed approach. {\textcopyright} 2017 American Society of Civil Engineers.},
author = {Abraham, Edo and Blokker, Mirjam and Stoianov, Ivan},
doi = {10.1061/(ASCE)WR.1943-5452.0000878},
file = {:home/dominik/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Abraham, Blokker, Stoianov - 2018 - Decreasing the Discoloration Risk of Drinking Water Distribution Systems through Optimized Topologic.pdf:pdf},
issn = {0733-9496},
journal = {Journal of Water Resources Planning and Management},
title = {{Decreasing the Discoloration Risk of Drinking Water Distribution Systems through Optimized Topological Changes and Optimal Flow Velocity Control}},
year = {2018}
}
@article{Lawyer2015,
abstract = {Centrality measures such as the degree, k-shell, or eigenvalue centrality can identify a network's most influential nodes, but are rarely usefully accurate in quantifying the spreading power of the vast majority of nodes which are not highly influential. The spreading power of all network nodes is better explained by considering, from a continuous-time epidemiological perspective, the distribution of the force of infection each node generates. The resulting metric, the under all primary epidemiological models across a wide range of archetypical human contact networks. expected force , accurately quantifies node spreading power Whennode power is low, influence is a function of neighbor degree. As power increases, a node'sowndegree becomes more important. The strength of this relationship is modulated by network structure, being more pronounced in narrow, dense networks typical of social networking and weakening in broader, looser association networks such as the Internet. The expected force can be computed independently for individual nodes, making it applicable for networks whose adjacency matrix is dynamic, not well specified, or overwhelmingly large.},
archivePrefix = {arXiv},
arxivId = {1405.6707},
author = {Lawyer, Glenn},
doi = {10.1038/srep08665},
eprint = {1405.6707},
file = {:home/dominik/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lawyer - 2015 - Understanding the influence of all nodes in a network.pdf:pdf},
isbn = {1091-6490 (Electronic)$\backslash$n0027-8424 (Linking)},
issn = {20452322},
journal = {Scientific Reports},
pmid = {25727453},
title = {{Understanding the influence of all nodes in a network}},
year = {2015}
}
@article{Golden,
author = {Golden, B and Bodin, L and Doyle, T and Stewart, W},
file = {:home/dominik/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Golden et al. - Unknown - Approximate Traveling Salesman Algorithms.pdf:pdf},
journal = {Operations Research},
number = {3},
pages = {694--711},
title = {{Approximate Traveling Salesman Algorithms}},
volume = {28},
year = {1980}
}